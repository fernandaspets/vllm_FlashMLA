#include "fwd.h"

#include <math_constants.h>
#include <cute/tensor.hpp>
#include <cutlass/cluster_launch.hpp>
#include <cutlass/arch/reg_reconfig.h>
#include <cutlass/arch/arch.h>
#include <cutlass/cuda_host_adapter.hpp>

#include "params.h"
#include "utils.h"
#include "sm120/ws_gemm.h"
#include "sm120/helpers.h"
#include "sm120/intrinsics.h"
#include <cutlass/gemm/collective/collective_builder.hpp>
#include <cutlass/gemm/collective/collective_builder.hpp>
#include <cutlass/gemm/kernel/gemm_universal.hpp>
#include <cutlass/epilogue/collective/collective_builder.hpp>
#include <cutlass/epilogue/collective/default_epilogue.hpp>
#include <cutlass/epilogue/thread/linear_combination.h>
#include <cutlass/gemm/dispatch_policy.hpp>
#include <cutlass/util/device_memory.h>
#include <cutlass/gemm_coord.hpp>

#include <cutlass/gemm/kernel/gemm_universal.hpp>
#include <cutlass/epilogue/collective/collective_builder.hpp>
#include <cutlass/epilogue/collective/default_epilogue.hpp>
#include <cutlass/epilogue/thread/linear_combination.h>
#include <cutlass/gemm/dispatch_policy.hpp>
#include <cutlass/util/device_memory.h>
#include <cutlass/gemm_coord.hpp>

#include "sm120/tma_cta_group2_nosplit.h"

namespace sm120 {

using namespace cute;

CUTE_DEVICE int32x8_t ldg_256_indices(void* src_ptr) {
    int32x8_t val;

#if (__CUDACC_VER_MAJOR__ > 12) || (__CUDACC_VER_MAJOR__ == 12 && __CUDACC_VER_MINOR__ >= 9)
    // CUDA 12.9+: single 256-bit load
    asm volatile("ld.global.nc.L1::evict_normal.L2::evict_normal.L2::256B.v8.s32 {%0, %1, %2, %3, %4, %5, %6, %7}, [%8];"
        : "=r"(val.a0), "=r"(val.a1), "=r"(val.a2), "=r"(val.a3),
          "=r"(val.a4), "=r"(val.a5), "=r"(val.a6), "=r"(val.a7)
        : "l"(src_ptr)
    );
#else
    // CUDA 12.8 and earlier: two 128-bit loads
    const char* base = static_cast<const char*>(src_ptr);

    asm volatile(
        "ld.global.nc.L1::evict_normal.L2::128B.v4.s32 {%0, %1, %2, %3}, [%4];\n"
        : "=r"(val.a0), "=r"(val.a1), "=r"(val.a2), "=r"(val.a3)
        : "l"(base)
    );

    asm volatile(
        "ld.global.nc.L1::evict_normal.L2::128B.v4.s32 {%0, %1, %2, %3}, [%4];\n"
        : "=r"(val.a4), "=r"(val.a5), "=r"(val.a6), "=r"(val.a7)
        : "l"(base + 16)
    );
#endif

    return val;
}

template<
    typename Shape_Q, typename TMA_Q,
    typename Shape_O, typename TMA_O
>

struct float2x2 {
    float2 lo, hi;
};

constexpr int D_Q = 576;
constexpr int D_K = 576;
constexpr int D_V = 512;
constexpr float MAX_INIT_VAL = -1e30;    // We use this number as the initial value for mi (max logits) to avoid -inf - (-inf) = nan

constexpr int B_H = 64;    // For SM120 2 CTAs (reduced for 99KB shared mem)
constexpr int B_TOPK = 64; // For SM120 2 CTAs (reduced for 99KB shared mem)
constexpr int NUM_BUFS = 2;
constexpr int NUM_THREADS = 128 + 128 + 64; // 128 TMA threads, 128 scale & exp threads, 32 UTCMMA threads

constexpr int D_sQ = 192, NUM_sQ_TILES = 3;
constexpr int D_tQ = D_Q - D_sQ, NUM_tQ_TILES = D_tQ / 64;
static_assert(D_sQ%64 == 0 && D_tQ%64 == 0 && D_sQ + D_tQ == D_Q);

// Tensor memory columns
namespace tmem_cols {
    //   0 ~ 256: output
    // 256 ~ 320: P
    // 320 ~ 512: Q[192:576]
    constexpr int o = 0;
    constexpr int p = 256;
    constexpr int q = 512 - D_tQ/2;
    static_assert(p+64 <= q);
}

template<int NUM_TILES>
using SmemLayoutQTiles = decltype(coalesce(tile_to_shape(
    UMMA::Layout_K_SW128_Atom<bf16>{},
    Shape<Int<B_H/2>, Int<64*NUM_TILES>>{},
    Step<_1, _2>{}
), Shape<_1, _1>{}));

template<int NUM_TILES>
using SmemLayoutOTiles = decltype(coalesce(tile_to_shape(
    UMMA::Layout_K_SW128_Atom<bf16>{},
    Shape<Int<B_H/2>, Int<64*NUM_TILES>>{},
    Step<_1, _2>{}
), Shape<_1, _1>{}));

using SmemLayoutO = SmemLayoutOTiles<8>;

template<int NUM_TILES>
using SmemLayoutKTiles = decltype(coalesce(tile_to_shape(
    UMMA::Layout_K_SW128_Atom<bf16>{},
    Shape<Int<B_TOPK/2>, Int<64*NUM_TILES>>{},
    Step<_1, _2>{}
), Shape<_1, _1>{}));

using SmemLayoutV = decltype(coalesce(tile_to_shape(
    UMMA::Layout_MN_SW128_Atom<bf16>{},
    Shape<Int<256>, Int<B_TOPK>>{},
    Step<_2, _1>{}
), Shape<_1, _1>{}));

template<int NUM_TILES>
using SmemLayoutSTiles = decltype(coalesce(tile_to_shape(
	UMMA::Layout_K_INTER_Atom<bf16>{},
	Shape<Int<B_H/2>, Int<64*NUM_TILES>>{},
	Step<_1, _2>{}
), Shape<_1, _1>{}));

struct SharedMemoryPlan {
    union {
        array_aligned<bf16, cosize_v<SmemLayoutQTiles<9>>> q_full;
        struct {
            array_aligned<bf16, cosize_v<SmemLayoutQTiles<NUM_sQ_TILES>>> sq;
            array_aligned<bf16, cosize_v<SmemLayoutV>> v;
            // NOTE K is not overlapped with q_full, so we can do k copy-in while performing S->T copy for q
            array_aligned<bf16, cosize_v<SmemLayoutKTiles<9>>> k;
        } s;
        array_aligned<bf16, cosize_v<SmemLayoutO>> o;
    } u;
    array_aligned<bf16, cosize_v<SmemLayoutSTiles<2>>> s;
    char is_k_valid[NUM_BUFS][B_TOPK/8];
    transac_bar_t bar_prologue_q, bar_prologue_utccp;
    transac_bar_t bar_qk_part_done[NUM_BUFS], bar_qk_done[NUM_BUFS];    // Pi = QKi^T done (i.e. Ki free)
    transac_bar_t bar_sv_part_done[NUM_BUFS], bar_sv_done[NUM_BUFS];    // O += SiVi done (i.e. Vi free)
    transac_bar_t bar_k_part0_ready[NUM_BUFS], bar_k_part1_ready[NUM_BUFS];
    transac_bar_t bar_v_part0_ready[NUM_BUFS], bar_v_part1_ready[NUM_BUFS];    // Vi is ready
    transac_bar_t bar_p_free[NUM_BUFS];
    transac_bar_t bar_so_ready[NUM_BUFS];   // S and O are ready
    transac_bar_t bar_k_valid_ready[NUM_BUFS], bar_k_valid_free[NUM_BUFS];
    array_aligned<uint32_t, 1> tmem_start_addr;
    float rowwise_max_buf[128], rowwise_li_buf[128];
};

// SM120: CUTLASS CollectiveBuilder for sparse attention
using ArchTag = cutlass::arch::Sm120;
using OperatorClassSparse = cutlass::arch::OpClassBlockScaledSparseTensorOp;
using OperatorClassDense = cutlass::arch::OpClassTensorOp;

// Tile shapes for SM120 (adjusted for 99KB shared memory)
// SM120 requires specific tile sizes (see CUTLASS 4.3.3 docs)
// For bf16 sparse: Use 128x128x128 tile for TN layout
using ThreadBlockShape = cute::Shape<cute::_128, cute::_128, cute::_256>;  // SM120 sparse requires 128x128x256
using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;  // SM120 fixed cluster

// SM120 only supports TN layout (RowMajor A, ColumnMajor B)
using LayoutA = cutlass::layout::RowMajor;
using LayoutB = cutlass::layout::ColumnMajor;
using LayoutC = cutlass::layout::RowMajor;
using LayoutD = cutlass::layout::RowMajor;

// Element types - bf16 for sparse attention
// SM120 sparse requires specific data types
using ElementA = cutlass::nv_float4_t<cutlass::float_e2m1_t>;
using ElementB = cutlass::nv_float4_t<cutlass::float_e2m1_t>;
using ElementC = cutlass::bfloat16_t;  // Output type
using ElementD = cutlass::bfloat16_t;  // Output type
using ElementAccumulator = float;

// Alignment requirements for SM120
constexpr int AlignmentA = 16;
constexpr int AlignmentB = 16;
constexpr int AlignmentC = 16;
constexpr int AlignmentD = 16;

// Schedule types for SM120 bf16 sparse GEMM
using KernelScheduleSparse = cutlass::gemm::KernelSparseTmaWarpSpecializedNvf4Sm120;
using KernelScheduleDense = cutlass::gemm::KernelSparseTmaWarpSpecializedNvf4Sm120;
using ThreadBlockShapeDense = ThreadBlockShape;
using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;  // SM120: single CTA

// Element types

// Schedule types for SM120

// SM120 CollectiveBuilder for sparse attention
// 1. Sparse Collective for Q*K^T

// SM120 Epilogue for sparse attention
using CollectiveEpilogueSparse = typename cutlass::epilogue::collective::CollectiveBuilder<
    ArchTag, OperatorClassSparse,
    ThreadBlockShape, ClusterShape,
    cutlass::epilogue::collective::EpilogueTileAuto,
    ElementAccumulator, ElementAccumulator,
    ElementC, LayoutC, AlignmentC,
    ElementD, LayoutD, AlignmentD,
    cutlass::epilogue::collective::EpilogueScheduleAuto
  >::CollectiveOp;

// SM120 CollectiveBuilder for sparse attention
// 1. Sparse Collective for Q*K^T

// Sparse Collective for Q*K^T (TN layout)
using CollectiveMainloopSparse = typename cutlass::gemm::collective::CollectiveBuilder<
    ArchTag, OperatorClassSparse,
    ElementA, LayoutA, AlignmentA,
    ElementB, LayoutB, AlignmentB,
    ElementAccumulator,
    ThreadBlockShape, ClusterShape,
    cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogueSparse::SharedStorage))>,
    KernelScheduleSparse
>::CollectiveOp;
